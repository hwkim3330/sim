# Korean Mini LM

ìëª¨(Jamo) ê¸°ë°˜ í•œêµ­ì–´ ë¯¸ë‹ˆ ì–¸ì–´ëª¨ë¸ - Pure JavaScript

## Features

- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ íŠ¹í™”** - ìëª¨ ë‹¨ìœ„ í† í°í™”ë¡œ ì‘ì€ vocabulary
- ğŸš€ **ì´ˆê²½ëŸ‰** - ~1MB ë¯¸ë§Œ (200K params ê¸°ì¤€)
- ğŸŒ **ë¸Œë¼ìš°ì € ì‹¤í–‰** - ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ
- ğŸ“¦ **No Dependencies** - Pure JavaScript

## Architecture

```
Input Text
    â†“
[Jamo Tokenizer] - í•œê¸€ â†’ ìëª¨ ë¶„ë¦¬
    â†“
[Embedding] - vocab_size Ã— embed_dim
    â†“
[GRU] - embed_dim â†’ hidden_dim
    â†“
[Dense] - hidden_dim â†’ vocab_size
    â†“
[Softmax] - í™•ë¥  ë¶„í¬
    â†“
Generated Token
```

## Model Size

| Config | Params | Size (Float32) |
|--------|--------|----------------|
| 64 Ã— 128 | ~50K | ~200KB |
| 128 Ã— 256 | ~200K | ~800KB |
| 256 Ã— 512 | ~800K | ~3MB |

## Quick Start

### Browser

```html
<script src="korean-mini-lm.js"></script>
<script>
  const { KoreanMiniLM } = window.KoreanMiniLM;

  const model = new KoreanMiniLM({
    embedDim: 128,
    hiddenDim: 256
  });

  // Generate text
  const text = model.generate('ì•ˆë…•', 50, 0.8);
  console.log(text);
</script>
```

### Node.js

```javascript
const { KoreanMiniLM } = require('./korean-mini-lm.js');

const model = new KoreanMiniLM();
console.log(model.generate('ì˜¤ëŠ˜ ë‚ ì”¨ê°€'));
```

## API

### KoreanMiniLM

```javascript
const model = new KoreanMiniLM({
  embedDim: 128,    // Embedding dimension
  hiddenDim: 256,   // GRU hidden dimension
});

// Generate text
model.generate(prompt, maxLength, temperature);

// Get model info
model.getInfo();
// { vocabSize: 77, embedDim: 128, hiddenDim: 256, paramCount: 206413, sizeMB: '0.79' }

// Save/Load (JSON)
const state = model.save();
model.load(state);

// Export/Import (Binary - smaller)
const buffer = model.exportBinary();
model.importBinary(buffer);
```

### JamoTokenizer

```javascript
const { JamoTokenizer } = window.KoreanMiniLM;
const tokenizer = new JamoTokenizer();

// Encode
const tokens = tokenizer.encode('ì•ˆë…•í•˜ì„¸ìš”');
// [2, 11, 0, 17, 8, 10, 5, 0, 11, 8, 1, 10, 20, 3]

// Decode
const text = tokenizer.decode(tokens);
// 'ì•ˆë…•í•˜ì„¸ìš”'
```

## Vocabulary

ìëª¨ ê¸°ë°˜ vocabulary (77 tokens):

| Category | Count | Examples |
|----------|-------|----------|
| Special | 10 | `<PAD>`, `<BOS>`, `<EOS>`, ` `, `\n` |
| ì´ˆì„± | 19 | ã„±, ã„´, ã„·, ã„¹, ã…, ... |
| ì¤‘ì„± | 21 | ã…, ã…“, ã…—, ã…œ, ã…¡, ã…£, ... |
| ì¢…ì„± | 27 | ã„±, ã„´, ã„·, ã„¹, ã…, ... |

## Training

í˜„ì¬ëŠ” forward passë§Œ êµ¬í˜„ë¨. ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„œëŠ”:

1. Backpropagation êµ¬í˜„ í•„ìš”
2. ë˜ëŠ” ì‚¬ì „ í•™ìŠµëœ weights ë¡œë“œ

```javascript
// ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì˜ˆì‹œ
fetch('pretrained.bin')
  .then(r => r.arrayBuffer())
  .then(buffer => {
    model.importBinary(buffer);
    console.log('Model loaded!');
  });
```

## Comparison

| Model | Size | Vocab | Device |
|-------|------|-------|--------|
| **Korean Mini LM** | ~1MB | 77 | Browser |
| TinyMistral-248M | 156MB | 32K | WebGPU |
| Qwen2-0.5B | 353MB | 152K | WebGPU |
| GPT-2 Small | 500MB | 50K | GPU |

## License

MIT
